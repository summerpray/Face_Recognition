# Face_recognition_NBUT

####项目简介：
- 本项目是基于宁波工程学院机器人学院的MADE2人脸识别项目
- 人脸识别采取PCA降维算法
- 本项目最后的aff_faces人脸集的识别正确率在96%左右
- 作者才疏学浅，如有错误请不吝指正

#### 项目文件下各文件的介绍
- PCA.py 主要算法 数据集的处理
- face_rec.py 识别脚本
- aff_faces文件夹/rar   原始人脸数据集
- feature_face_all.npy 存储所有人脸在特征向量的基上的投影
- feature_vector.npy   存储特征向量构成的一组基
- label_rec.npy/label_train.npy 存储识别/训练对应人脸标签
- mat_rec.npy/mat_train.npy 存储人脸识别/训练集的矩阵
- meanMatrix.npy 存储平均矩阵(用来去中心化)
- X.npy 存储原始矩阵(多余) 

## 更新日志
#### 2020.1.4
- 项目开展
- 搭建了整体框架

#### 2020.1.5
- 人脸识别PCA数学原理
- 完善PCA算法
- 更新框架

#### 2020.1.6
- 更新了数据及图像读取处理，每个人脸取七张作为训练数据
- 添加了每个人脸的特征脸的生成方式
- 完成项目
- 最终的识别率为95.8%

#### 2020.1.8
- 更新了人脸识别方法稀疏表示
- 识别成功率在90%左右
- 更新了人脸识别方法基于PCA的稀疏表示
- 识别成功率介于PCA与稀疏表示之间

#### 2020.1.9
- 编者深层次的理解了一下稀疏表示分类法（SRC）并且在代码中已经更新了但是还是有一个问题(方程是否可以用最小二乘法求得最小二乘解)
- 请读者自行理解范数以及范数在及其学习中的意义

#### 2020.1.10
- 更新了数据处理和识别率曲线图

#### 2020.1.11
- 更新了项目报告，报告中含有SRC,LDA,PCA,SVD的数学推导
- 本项目中并没有SVD与LDA的代码，读者需要请了解数学原理后自行编写

##### 本项目用时大约60小时 但主要是各个算法的数学原理分析需要牢牢掌握

### 后记
- 最小二乘法并不适用于SRC分类器，是因为首先训练集构成了N维的线性空间，那么我们又有一个处于线性空间外的向量X（测试样本），假如我们做这个空间的最小二乘解，那么其实我就有了一个测试样本在训练样本空间的投影，但是这个投影在线性空间中必然能被其他的非当类解的向量线性表示出来，这就违背了我们一开始使解最稀疏的目标。

以下是项目报告

## **人脸识别项目设计**

>   Author: **summerpray**


2021年1月11日

摘 要


人脸识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部识别的一系列相关技术，通常也叫做人像识别、面部识别。本项目基于人脸识别的研究背景，通过了解使用相关的线性代数、概率和数理统计知识，达到项目预期目的。项目报告共分为三大部分，在报告开头首先将介绍项目运行的总体工作原理；其次，将基于人脸识别详细介绍相关理论知识，如何使用相关算法以及在理论与实验中的操作情况；最后，将对整体项目进行完善与总结。
**关键词**：人脸识别；线性代数；工作原理；

ABSTRACT

Face recognition is a kind of biometric identification technology based on human
face feature information. A series of related technologies that collect images
or video streams containing faces with cameras or cameras, and automatically
detect and track faces in the images, and then carry out face recognition on the
detected faces, usually also known as portrait recognition and facial
recognition. Based on the research background of face recognition, this project
achieves the expected purpose of the project by understanding and using relevant
knowledge of linear algebra, probability and mathematical statistics. The
project report is divided into three parts. At the beginning of the report, the
general working principle of the project operation will be introduced. Secondly,
based on face recognition, the relevant theoretical knowledge will be introduced
in detail, how to use the relevant algorithms and the operation in theory and
experiment; Finally, the overall project will be improved and summarized.

**Key words**: face recognition; Linear Algebra; Working principle;

>   **目录**

[1	项目简介	6](#_Toc61171911)

[2	项目设计流程	7](#_Toc61171912)

>   [**2.1**	**项目设计思路**	7](#_Toc61171913)

>   [2.1.1项目设计总体思路图如下	7](#_Toc61171914)

>   [**2.2**	**基于主成分分析（PCA）人脸识别原理**	7](#_Toc61171915)

>   [2.2.1 PCA简介	7](#_Toc61171916)

>   [2.2.2 内积 基	8](#_Toc61171917)

>   [2.2.3 方差	8](#_Toc61171918)

>   [2.2.4 拉格朗日乘子法	9](#_Toc61171919)

>   [2.2.5 协方差	10](#_Toc61171920)

>   [2.2.6 协方差矩阵	11](#_Toc61171921)

>   [2.2.7 降维	13](#_Toc61171922)

>   [**2.3**	**奇异值分解（SVD）**	13](#_Toc61171923)

>   [2.3.1 奇异值分解定义	13](#_Toc61171924)

>   [2.3.2 奇异值求解	13](#_Toc61171925)

>   [**2.4**	**PCA算法两种实现方法**	15](#_Toc61171926)

>   [2.4.1 基于特征值分解协方差矩阵实现PCA算法	15](#_Toc61171927)

>   [2.4.2 基于SVD分解协方差矩阵实现PCA算法	15](#_Toc61171928)

>   [**2.5**	**线性判别分析（LDA）**	16](#_Toc61171929)

>   [2.5.1 LDA简介	16](#_Toc61171930)

>   [2.5.2 均值差	16](#_Toc61171931)

>   [2. 5. 3 散列值	17](#_Toc61171932)

>   [2.5.4 类内散度矩阵 类间散度矩阵	18](#_Toc61171933)

>   [2.5.6 拉格朗日乘子法确定	18](#_Toc61171934)

>   [**2.6**	**稀疏表示分类法（SRC）**	18](#_Toc61171935)

>   [2.6.1 稀疏表示	18](#_Toc61171944)

>   [2.6.2 稀疏表示分类法(SRC)	19](#_Toc61171945)

>   [2.6.3 Ax = Y的解和对人脸识别算法优化的思考	19](#_Toc61171946)

>   [**2.7**	**性能比较**	20](#_Toc61171947)

>   [2.7.1 特征值分解与SVD	22](#_Toc61171948)

>   [**2.7.2 PCA与LDA**	23](#_Toc61171949)

>   [**2.7.3 PCA与SRC**	24](#_Toc61171950)

[参考文献	26](#_Toc61171951)

1.  项目简介

人脸识别(Face
recognition)，宁波工程学院机器人学院MADE2项目，线性代数实际应用项目，主要应用PCA、SVD、神经网络等算法。人脸识别是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部识别的一系列相关技术，通常也叫做人像识别、面部识别。

人脸识别目前流行的算法有：1、基于人脸特征点的识别算法（Feature-based recognition
algorithms）。2、基于整幅人脸图像的识别算法（Appearance-based recognition
algorithms）。3、基于模板的识别算法（Template-based recognition
algorithms）。4、利用神经网络进行识别的算法（Recognition algorithms using neural
network）。在本项目中主要应用了PCA、SVD算法来进行人脸识别。主要的实现语言有python、matlab。

1.  项目设计流程

    1.  **项目设计思路**


（在整个项目过程中图片的特征提取将作为一个重点内容，也是项目的核心，进下来将主要介绍图片特征提取的几种方法。）

1.  **基于主成分分析（PCA）人脸识别原理**

2.2.1 PCA简介

>   PCA（Principal Component Analysis）是一种常见的数据分
>   析方式，常用于高维数据的降维，可用于提取数据的主要特征分量，是一种使用广泛的数据降维算法，PCA的主要思想是将n维特征映射到k维，实现矩阵的降维。

2.2.2 内积 基


>   基：在之前的线性代数学习过程中，我们知道一个线性空间可以用一组基来表示，即线性空间中所有的元素都能用这组基线性表示。所以，当求一个向量在一组基的坐标时，我们将这个向量分别和这组单位化的基求内积即可。而对于一个向量，如果我们要准确的描述它，首先要确定一组基，并且这组基向量模长为1，然后求出所在基上的投影即可。

2.2.3 方差

>   方差：在进行图像降维时，我们要尽可能的保留有效数据，即使数据在一组基上的投影更分散，从而不会使原始数据缺失。而在数学上描述数据分散程度的便是方差。一个变量的方差可以看作是每个元素与变量均值差的平方和的均值，即

>   为了方便处理，将变量的均值都化为0，因此方差可以直接用每个元素的平方和除以元素个数表示

>   所以这个问题可以建立优化模型即：寻找一组基，使得所有数据变换为这个基上的坐标表示后，方差值最大。

>   方差的几何理解，举上图向量为例，当变量的均值都化为0后，向量在上的方差为（为在上的投影），即转化为在上的坐标的平方。对于一组向量{}在一组单位化的向量上的投影分别为

>   写成矩阵为

>   即这组向量在上的方差为

2.2.4 拉格朗日乘子法

>   为使投影后的数据更为分散，要求方差为最大。由上可知，一组向量在单位化后的基上的方差为，因为。可得

>   令，S为协方差矩阵。

>   因为

>   构建拉格朗日函数

>   即可得到

>   由该式子可以得出当为的特征向量时，方差最大。

2.2.5 协方差

>   我们已经得知方差的计算方法，并且知道它的几何意义。如图1-1所示，在经过基变换之后，点D投影到了基向量V上，那么在新的基上的方差大小应该是投影坐标到基向量零点的距离的平方，即：

>   那么我们将展开,得到：

>   可以看到，其实就是协方差矩阵。


>   了解了协方差矩阵后，来了解一些协方差，首先，协方差（去均值）的公式：

>   因为方差只是对于一维空间来说，而对于高维数据，我们要用协方差进行约束，协方差可以表示两个变量的相关性，而为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。

当协方差为0的时候，表示两个变量线性无关，而为了达到这个目的，我们在选择第二个基的时候只能在与第一个基正交方向上选择。

2.2.6 协方差矩阵

那么重新看回 ，D是原始数据矩阵

我们发现：

协方差矩阵C是一个对称矩阵，那么意味着C可以正交对角化，即协方差矩阵C的特征向量构成的矩阵，那么

其中P是属于C的单位特征向量构成的矩阵，E是属于C的特征值构成的对角矩阵，这样协方差矩阵C的对角化过程就完成了，那么我们把它回带回原来的方差中：

可以发现，E是不变的，那么我们使方差最大的目标就转变成了使最大，可以表示成P在基上的投影最大，不难得知，当基和P一致的时候，即有：的时候，投影最大。我们又可以通过对称矩阵的性质得知属于不同特征空间的任意两个特征向量是正交的，那么我们所要求的降维后的基，就是协方差矩阵C的特征向量构成的基。为了达到降维的效果，我们要先对特征值的对角矩阵和对应的特征向量矩阵从大到小排序，特征值越大表示数据在这个特征向量方向拉伸越大，对应到最后计算的方差就越大，所以最后要尽量取前K个最大的特征值的同时使数据损失达到最小。根据特征值的贡献率，我们选取前p（p\<m）个最大特征值及其对应的特征向量。

贡献率是指选取的特征值的和与占所有特征值的和比，即：

至此，我们已经得到了可以将数据降维到同一个维度的一组基P。

2.2.7 降维

我们已经求得了一组基，那么就可以求得最开始的数据矩阵。这组基上的投影，也就是说，可以把原来的高维矩阵，降维到更加低维的特征空间中并且尽可能保留原始数据，至此，我们已经把最开始的数据处理完毕。

1.  **奇异值分解（SVD）**

2.3.1 奇异值分解定义

对于一个的实数矩阵A，我们想要把它分解成如下形式

其中U和V均为单位正交矩阵，既有，U称为左奇异矩阵，V称为右奇异矩阵，仅在对角线上有值，我们称它为奇异值，其他元素为0。奇异值的分解过程可以用如下图形象的表示。


2.3.2 奇异值求解

首先证明和有相等的特征值,证明如下

所以和有相等的特征值

证明的秩与的秩相同，即 的零空间的位数相同：

所以可以由推出

所以可以由推出

所以的秩与的秩相同

因为证明特征值都大于0（只有大于0才能进行开方）

所以是半正定矩阵，所以特征值都大于0。

所以对于实对称矩阵，必然存在

>   通过计算可以发现虽然和的维数不相同，但是它们在主对角线的奇异值是相等的。即特征值为或中特征值的开方。

1.  **PCA算法两种实现方法**

2.4.1 基于特征值分解协方差矩阵实现PCA算法

输入：训练图片矩阵A（），降维到k

1、去平均值（中心化矩阵）

2、算协方差矩阵

3、用特征值分解方法求协方差矩阵的特征向量与特征值

4、对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。

5、将数据转换到k个特征向量构建的新空间中，即Y=PX.

2.4.2 基于SVD分解协方差矩阵实现PCA算法

输入：训练图片矩阵A（），降维到k

1、去平均值（中心化矩阵）

2、计算协方差矩阵

3、通过SVD计算协方差矩阵的特征值与特征向量。

4、对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。

5、将数据转换到k个特征向量构建的新空间中。

1.  **线性判别分析（LDA）**

2.5.1 LDA简介

即寻找一个最佳向量，使类内样本投影到向量之后分散程度最小，类间样本投影到向量之后分散程度最大。

2.5.2 均值差

对于一个给定的数据集，在考虑到，找到一个向量，使几类样本，经过投影之后，分散度最大，在这里，我们先定义一个均值向量（为了方便处理，在这里我们取两类数据进行说明），即

由于是两类数据，我们将这两类数据的中心点投影到向量上，使两个中心点分散程度最大，即均值差最大

2\. 5. 3 散列值

但是若只考虑均值差，容易导致，均值差达到最大，但是类间会有重叠部分，因此，为避免这种情况的发生，我们通过散列值来进行约束，即使类内的分散度达到最小。

所以，综合上述两者，我们将和同时来度量我们所需要的最佳向量

将问题转变为寻找使最大的向量。（称为散列矩阵）

2.5.4 类内散度矩阵 类间散度矩阵

类内散度矩阵：为类内样本值到均值之间的距离。

类间散度矩阵：类间中心点之间的距离

即

2.5.6 拉格朗日乘子法确定

由于，分子与分母均为二次式，为了方便计算可将假设为1，构建拉格朗日函数

因为和是对称矩阵，即

所以，是的特征向量。

2.6 **稀疏表示分类法（SRC）**

2.6.1 稀疏表示

什么是稀疏表示？假设我有一本汉语字典，我又有一首诗，那么我这首诗中的每一个字都能在字典中找到，但是这些字在字典中的分布肯定是很稀疏的。稀疏表示通俗的讲就是用最少的字去描述一件事，比如：我十分饿想吃点东西，如果用稀疏表示来重组这句话，那就是“我想吃饭”。所以我们类比到一个多维空间中，一个N维的向量，我可以用无穷个向量去线性表示这个向量，但是一旦我用稀疏表示来表示这个向量，我最稀疏的表示方法必然是用他的N维基向量去表示他。

2.6.2 稀疏表示分类法(SRC)

我们同样可以将它类比到我的人脸识别中。假定有来自C个类的N个训练样本，其中有n个训练样本来自第K类。

那么第K类的任何测试样本都可以近似的表示为该类的训练样本的线性组合

那么对于全部的训练样本A来说

其中A是由c个类的所有n个训练样本组成的矩阵。 那么它的解X必然可以表示为

X向量是系数向量，仅与之相关联的第K个类的系数是非0的。那么当c很大时，非零解的个数在这个总数中就会很稀疏。为了方便，我们可以将解向量也划分成c类

2.6.3 Ax = Y的解和对人脸识别算法优化的思考

#### 2.6.3.1 最小二乘法

其实我们可以发现，在求解过程中我们有一个前提，就是说这张测试样本y一定能被原属于他那第K类的照片线性表示，但如果：

方程是不相容的，就只能求出最小二乘解，即：

那就意味着如果全部的训练样本集足够大，理论上来说我可以用一整个样本集中的一些照片来线性表示他，但是最后的相对大的系数的解集总应该集中于第K类的解向量中，也就是说，原解中应该等于0的地方有了一些噪音，这些噪音是分布不均的，这意味的如果噪音过大，大于原本的正确解，那么识别就会出现误差。

所以对于算法的优化思路是求出每一类的平均解的系数，用最大的类的系数来确定测试样本与哪个类最相似，这样就从测试样本与哪张照片最相似转变成测试样本与哪个类最相似的问题，降低了识别误差，增加了识别的正确率。

2.6.3.2 **范数和对最小零范数求解的方法**

还有一种求法，但是是建立在方程相容或者第K类样本能**近似**线性表示测试样本，那么只要满足以下条件就可以做出目标最优解：

然而，找到上式的稀疏解是NP-hard：即，现阶段没有比穷举x的所有子集更有效的获得稀疏解的方法。
[稀疏表示](https://baike.baidu.com/item/%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA)和[压缩感知](https://baike.baidu.com/item/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5)理论揭示了我们可以解决以下[凸优化](https://baike.baidu.com/item/%E5%87%B8%E4%BC%98%E5%8C%96)，也就是说，在我们这个问题中可以用最小一范数等价最小零范数求解问题，以获得近似解：

![preview](media/54ae812db0a63551c958cda734e43e75.jpeg)如何理解L1范数是L0范数的最优凸近似？

我们可以从以上的图中可以了解到：

1、AX=b：表示空间中的一条直线

2、\|\|X\|\|p：表示空间中的Lp球；

>   3、当0\<p\<1时，Lp球是内凸的，当球的半径逐渐增加时与直线的交点将位于坐标轴上，而坐标轴上的点是稀疏的（除了该所在坐标轴的坐标值不为0外，其他均为0）；

>   4、当p=1时，Lp球是菱状，在一定条件下会导致一个稀疏解，即相交于坐标轴上；（这也许是压缩感知中L0模型在一定条件下等价于L1模型的形象解释吧）

>   5、当p\>1时，Lp球是外凸的，当逐渐膨胀时与直线的切点一定不位于坐标轴上，即此时的解是不稀疏的。如图中的L2球。

当然，对于以上的解释我们可以通俗的了解到：

涉及到求X的一范数，X的一范数可以表示为

那么假设在二维空间也就是笛卡尔坐标系中，一范数就代表着一个原点对称的平行四边形，那就可以用拉格朗日乘子来进行证明：

从中可知，当两边相等的时候取到极值，即两边的函数图像有交点的时候才能取到最小值，那么如果不管右边的函数图像是什么样的，最有可能有解的时候就是函数图像与菱形的端点相交的时候，就是我们求得最稀疏解的情况。

为什么相交在坐标轴上就是稀疏了呢？首先不要忘了我们的初衷，是L0范数，就是非零元素的个数，也就是最稀疏的解，对于几何表示的二维空间，要满足AX=b的约束条件，最稀疏也就是两个元素x，y中一个为零，一个非零。如果全为零，则为AX=0的齐次解，就不再满足AX=b的约束条件。所以稀疏解就是直线与横轴或纵轴的交点了。另外也可以从另一个更简单的角度来考虑，L0范数的解就是在坐标轴上的，所以L1要想等效L0范数，解就需要也在坐标轴上，仅此而已。

1.  **性能比较**

2.7.1 特征值分解与SVD

1.在数据处理时间上

在PCA降维过程中，需要找到样本协方差矩阵的最大的个特征向量，然后用这最大的个特征向量张成的矩阵来做低维投影降维。在这个过程中首先需要先求出协方差矩阵，当样本数多样本特征数也多的时候，这个计算量是很大的。本项目中是一个的矩阵，在计算其特征向量与特征值时，需要花费较长的时间。

而用SVD处理同时也可以得到协方差矩阵最大的个特征向量张成的矩阵，并且SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵，也能求出我们的右奇异矩阵V这在数据处理的方面提供了很大的便利程度。也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效，且在数据处理时，能花较少的时间，来实现目标值。

2.在识别率上

特征值分解，我们所选取的前个特征向量，满足贡献率为，（我们选取200张图片为训练样本，200张图片为测试样本）在测试中发现，满足贡献率为的特征向量个数为71个，识别率达到。

同样，使用SVD分解时，我们选取前71个特征向量进行计算，在运算结果显示，识别率达到，与特征值分解的识别率一致。

总结：由以上两个方面，我们可以看出在识别率上，特征值分解与SVD几乎相同，但是在算法运算过程中，SVD的运行速度大大超过特征值分解。

**2.7.2 PCA与LDA**

1.在原理上

PCA是一种正交投影，即将原始数据投影到一组向量上，时投影之后的数据尽可能的分散（即较好的保留原始数据），从而实现特征值的提取，以达到矩阵的降维。

LDA是以类来做区分，即将原始数据的不同类别进行划分，将数据投影到一组向量之后，使得类与类之间的分散程度较大，而类内之间的样本较为集中，从而实现类与类的区分，以达到矩阵的降维。

2.在识别率上

**2.7.3 PCA与SRC**

1、PCA只是一个降维算法，比较相似照片还是通过求取向量间的欧氏距离也就是二范数来实现的，但是SRC算法是通过样本类的线性组合直接表示出测试样本，这意味着SRC可以实现更加复杂的人脸识别，也就是有遮挡的人脸识别。但同样的，SRC处理训练样本不可避免的有很多噪点，这必然会导致识别率的下降，而PCA在降维的过程中也是一个去噪的过程，这也增加了识别率。

2、尝试了PCA与SRC的结合，先通过PCA降维，用降维后的特征向量再进行SRC的识别，结果发现识别率比SRC要高，但是比PCA要低，这不免引起了我们的思考。但最后编者能想出的最合理的解释是这样的：经过PCA的降维后损失了一些特征基，但是这些特征基或许是线性表示测试样本的重要组成部分，导致最后SRC的近似解误差变大，所以识别率比PCA要低。但是降维的过程又是一个降噪的过程，所以失去了一些噪点之后识别准确率要比SRC要高。

结 论

人脸识别是基于线性代数的一个创新性项目。在这一过程中，使用了大量线性代数、概率等知识，从开始的理论知识，到后面的实际操作。MADE2项目很好的诠释了将数学应用于实际生活这一道理。

在整个项目过程中，不仅仅是个人能力的提升，更重要的是团队成员之间的相互配合与分工。从PCA的特征值分解到SVD实现优化，再到后面的LDA与SRC，一系列的算法与数学原理，通过自主学习，团队之间的交流，逐步的完善，对于人脸识别的整个框架，有更进一步的了解与深入，为以后的学习奠定了良好的基础。

>   参考文献

[1]张娜,刘坤,韩美林,陈晨.一种基于PCA和LDA融合的人脸识别算法研究[J].电子测量技术,2020,43(13):72-75.

[2]张迪. 图像稀疏表示的K-SVD算法及其在人脸识别中的应用研究[D].西北师范大学,2020.

[3]沈理,刘翼光,熊志勇. 人脸识别原理及算法[M].人民邮电出版社:, 201410..

[4]雷文华. 基于特征融合的人脸识别方法研究[D].西安科技大学,2020.

[5]苑军科. 基于PCA的人脸识别系统的设计与研究[D].河北科技大学,2019.

[6]吕芳芳.基于子空间的人脸识别方法的分析与研究[J].电脑知识与技术,2020,16(33):185-186+194.

[7]张泓,范自柱,王松,李争名.特征空间中的拓展稀疏人脸识别[J].重庆大学学报,2020,43(11):21-28.

[8]李子奇. 基于稀疏表示的图像分类算法研究[D].江南大学,2020.

[9]刘伟. 基于稀疏表示的人脸识别算法研究[D].江南大学,2020.